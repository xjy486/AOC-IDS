{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix, precision_score, recall_score, f1_score\n",
    "import scipy.optimize as opt\n",
    "import torch.distributions as dist\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "超参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数\n",
    "tem = 0.02\n",
    "bs = 128\n",
    "seed = 5009\n",
    "epochs = 800\n",
    "epoch_online=1\n",
    "sample_interval = 2784\n",
    "flip_percent = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    train_data=pd.read_csv('en_KDDTrain+.csv')\n",
    "    test_data=pd.read_csv('en_KDDTest+.csv')\n",
    "    y_train=train_data['label2']\n",
    "    y_test=test_data['label2']\n",
    "    X_train=train_data.drop(columns=['label2','class','label'])\n",
    "    X_test=test_data.drop(columns=['label2','class','label'])\n",
    "    normalize=MinMaxScaler()\n",
    "    X_train=normalize.fit_transform(X_train)\n",
    "    X_test=normalize.fit_transform(X_test)\n",
    "    return X_train,y_train,X_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train,x_test,y_test=get_dataset()\n",
    "# 转换为torch张量\n",
    "x_train,y_train=torch.FloatTensor(x_train).to(device),torch.LongTensor(y_train).to(device)\n",
    "x_test,y_test=torch.FloatTensor(x_test).to(device),torch.LongTensor(y_test).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([125973, 121]),\n",
       " torch.Size([125973]),\n",
       " torch.Size([22544, 121]),\n",
       " torch.Size([22544]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape,y_train.shape,x_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y,y_pred):\n",
    "    y= y.cpu().detach().numpy()\n",
    "    y_pred= y_pred.cpu().detach().numpy()\n",
    "    # 混淆矩阵\n",
    "    print(\"Confusion matrix\")\n",
    "    print(confusion_matrix(y, y_pred))\n",
    "    # Accuracy \n",
    "    print('Accuracy ',accuracy_score(y, y_pred))\n",
    "    # Precision \n",
    "    print('Precision ',precision_score(y, y_pred))\n",
    "    # Recall\n",
    "    print('Recall ',recall_score(y, y_pred))\n",
    "    # F1 score\n",
    "    print('F1 score ',f1_score(y,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AE自编码器模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(AE, self).__init__()\n",
    "        # 计算输入维度的最近的2的幂次方，比如输入维度是206，则最近的2的幂次方是128\n",
    "        nearest_power_of_2 = 2 ** round(math.log2(input_dim))\n",
    "        \n",
    "        second_fourth_layer_size = nearest_power_of_2 // 2\n",
    "        third_layer_size = nearest_power_of_2 // 4\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, second_fourth_layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(second_fourth_layer_size, third_layer_size)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(third_layer_size, second_fourth_layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(second_fourth_layer_size, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRC对比损失模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRCLoss(nn.Module):\n",
    "    def __init__(self, device, temperature=0.1, scale_by_temperature=True):\n",
    "        super(CRCLoss, self).__init__()\n",
    "        self.device = device\n",
    "        self.temperature = temperature\n",
    "        self.scale_by_temperature = scale_by_temperature\n",
    "\n",
    "    def forward(self, features, labels=None):\n",
    "        # 计算特征的归一化表示\n",
    "        features = F.normalize(features, p=2, dim=1)\n",
    "        batch_size = features.shape[0]\n",
    "        num_norm = len(labels==0)\n",
    "        ## contiguous方法确保张量在内存是连续存储的，因为变换视图操作需要确保张量在内存中是连续存储的\n",
    "        labels = labels.contiguous().view(-1, 1) # batch_size * 1\n",
    "        \n",
    "        if labels.shape[0] != batch_size:\n",
    "            raise ValueError('Batch size of features and labels do not match.')\n",
    "        \n",
    "        # 计算余弦相似度，cosine_sim[i][j]表示features[i]和features[j]的余弦相似度，矩阵大小为batch_size * batch_size\n",
    "        cosine_sim = torch.nn.functional.cosine_similarity(features.unsqueeze(1), features.unsqueeze(0), dim = -1)/self.temperature\n",
    "        # 将余弦相似度的对角线元素设置为0\n",
    "        mask_diag = torch.eye(batch_size, dtype=torch.bool)\n",
    "        cosine_sim[mask_diag] = 0\n",
    "        # 正样本对\n",
    "        sim_pos = cosine_sim[(labels==0).squeeze()]\n",
    "        # 正样本对间的余弦相似度\n",
    "        sim_pos_pos = sim_pos[:,(labels==0).squeeze()] \n",
    "        # 正样本与负样本间的余弦相似度\n",
    "        sim_pos_neg = sim_pos[:,(labels==1).squeeze()]\n",
    "        # 计算正样本与负样本间的分数和\n",
    "        sum_pos_neg = torch.sum(torch.exp(sim_pos_neg))\n",
    "        # 计算分母\n",
    "        denominator = torch.exp(sim_pos_pos) + sum_pos_neg       \n",
    "        loss = -(sim_pos_pos-torch.log(denominator))  \n",
    "        \n",
    "        \n",
    "        if self.scale_by_temperature:\n",
    "            loss = loss * self.temperature\n",
    "        # 计算损失\n",
    "        loss= loss.mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADM自主决策"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_pdf(x, mean, std):\n",
    "    pdf = 1/(std*np.sqrt(2*np.pi))*np.exp(-(x-mean)**2/(2*std**2))\n",
    "    return pdf\n",
    "def log_likelihood(params,data):\n",
    "    data = data.cpu().detach().numpy()\n",
    "    mean_pos_enc, std_pos_enc, mean_neg_enc, std_neg_enc = params\n",
    "    pdf1 = gaussian_pdf(data, mean_pos_enc, std_pos_enc)\n",
    "    pdf2 = gaussian_pdf(data, mean_neg_enc, std_neg_enc)\n",
    "    mixture_pdf = 0.5*pdf1 + 0.5*pdf2\n",
    "    log_likelihood = -np.sum(np.log(mixture_pdf))\n",
    "    \n",
    "    return log_likelihood\n",
    "def predict(norm_enc, norm_dec, x_train, y_train, x_test, model):\n",
    "    x_train_pos=x_train[(y_train==0).squeeze()]\n",
    "    x_train_neg=x_train[(y_train==1).squeeze()]\n",
    "    \n",
    "    # 训练集全体样本编码和解码特征\n",
    "    train_enc = F.normalize(model(x_train)[0], p=2, dim=1)\n",
    "    train_dec = F.normalize(model(x_train)[1], p=2, dim=1)\n",
    "    # 训练集正样本编码和解码特征\n",
    "    train_enc_pos = F.normalize(model(x_train_pos)[0], p=2, dim=1)\n",
    "    train_dec_pos = F.normalize(model(x_train_pos)[1], p=2, dim=1)\n",
    "    # 训练集负样本编码和解码特征\n",
    "    train_enc_neg = F.normalize(model(x_train_neg)[0], p=2, dim=1)\n",
    "    train_dec_neg = F.normalize(model(x_train_neg)[1], p=2, dim=1)\n",
    "    \n",
    "    # 分别计算训练集正样本特征和平均正样本特征的余弦相似度 负样本同理 全体样本同理 \n",
    "    sim_pos_norm_enc = F.cosine_similarity(train_enc_pos, norm_enc.unsqueeze(0), dim=1) \n",
    "    sim_pos_norm_dec = F.cosine_similarity(train_dec_pos, norm_dec.unsqueeze(0), dim=1)\n",
    "    sim_neg_norm_enc = F.cosine_similarity(train_enc_neg, norm_enc.unsqueeze(0), dim=1)\n",
    "    sim_neg_norm_dec = F.cosine_similarity(train_dec_neg, norm_dec.unsqueeze(0), dim=1)\n",
    "    sim_all_norm_enc = F.cosine_similarity(train_enc, norm_enc.unsqueeze(0), dim=1)\n",
    "    sim_all_norm_dec = F.cosine_similarity(train_dec, norm_dec.unsqueeze(0), dim=1)\n",
    "    # 进行排序\n",
    "    sort_sim_pos_norm_enc, indices = torch.sort(sim_pos_norm_enc)\n",
    "    sort_sim_pos_norm_dec, indices = torch.sort(sim_pos_norm_dec)\n",
    "    sort_sim_neg_norm_enc, indices = torch.sort(sim_neg_norm_enc)\n",
    "    sort_sim_neg_norm_dec, indices = torch.sort(sim_neg_norm_dec)\n",
    "\n",
    "    \n",
    "    ## 初始化参数\n",
    "    mean_pos_enc = torch.mean(sort_sim_pos_norm_enc).cpu().detach().numpy()\n",
    "    std_pos_enc = torch.std(sort_sim_pos_norm_enc).cpu().detach().numpy()\n",
    "    mean_pos_dec = torch.mean(sort_sim_pos_norm_dec).cpu().detach().numpy()\n",
    "    std_pos_dec = torch.std(sort_sim_pos_norm_dec).cpu().detach().numpy()\n",
    "    mean_neg_enc = torch.mean(sort_sim_neg_norm_enc).cpu().detach().numpy()\n",
    "    std_neg_enc = torch.std(sort_sim_neg_norm_enc).cpu().detach().numpy()\n",
    "    mean_neg_dec = torch.mean(sort_sim_neg_norm_dec).cpu().detach().numpy()\n",
    "    std_neg_dec = torch.std(sort_sim_neg_norm_dec).cpu().detach().numpy()\n",
    "    initial_params_enc = [mean_pos_enc, std_pos_enc, mean_neg_enc, std_neg_enc]\n",
    "    initial_params_dec = [mean_pos_dec, std_pos_dec, mean_neg_dec, std_neg_dec]\n",
    "    # 拟合高斯分布\n",
    "    fit_enc = opt.minimize(log_likelihood, initial_params_enc, args=(sim_all_norm_enc,), method='Nelder-Mead')\n",
    "    fit_dec = opt.minimize(log_likelihood, initial_params_dec, args=(sim_all_norm_dec,), method='Nelder-Mead') \n",
    "    # print(\"enc init\")\n",
    "    # print(initial_params_enc)\n",
    "    # print(\"dec init\")\n",
    "    # print(initial_params_dec)\n",
    "    mean1_enc, std1_enc, mean2_enc, std2_enc = fit_enc.x\n",
    "    mean1_dec, std1_dec, mean2_dec, std2_dec = fit_dec.x\n",
    "    # print(\"encoder:\")\n",
    "    # print(mean1_enc, std1_enc, mean2_enc, std2_enc)\n",
    "    # print(\"decoder:\")\n",
    "    # print(mean1_dec, std1_dec, mean2_dec, std2_dec)\n",
    "    # 选择均值小的作为正常样本的均值\n",
    "    if mean1_enc < mean2_enc:\n",
    "        mean_pos_enc, mean_neg_enc = mean1_enc, mean2_enc\n",
    "        std_pos_enc, std_neg_enc = std1_enc, std2_enc\n",
    "        gaussian_pos_enc = dist.Normal(mean_pos_enc, std_pos_enc)\n",
    "        gaussian_neg_enc = dist.Normal(mean_neg_enc, std_neg_enc)\n",
    "    else:\n",
    "        mean_pos_enc, mean_neg_enc = mean2_enc, mean1_enc\n",
    "        std_pos_enc, std_neg_enc = std2_enc, std1_enc\n",
    "        gaussian_pos_enc = dist.Normal(mean_pos_enc, std_pos_enc)\n",
    "        gaussian_neg_enc = dist.Normal(mean_neg_enc, std_neg_enc)\n",
    "    if mean1_dec < mean2_dec:\n",
    "        mean_pos_dec, mean_neg_dec = mean1_dec, mean2_dec\n",
    "        std_pos_dec, std_neg_dec = std1_dec, std2_dec\n",
    "        gaussian_pos_dec = dist.Normal(mean_pos_dec, std_pos_dec)\n",
    "        gaussian_neg_dec = dist.Normal(mean_neg_dec, std_neg_dec)\n",
    "    else:\n",
    "        mean_pos_dec, mean_neg_dec = mean2_dec, mean1_dec\n",
    "        std_pos_dec, std_neg_dec = std2_dec, std1_dec\n",
    "        gaussian_pos_dec = dist.Normal(mean_pos_dec, std_pos_dec)\n",
    "        gaussian_neg_dec = dist.Normal(mean_neg_dec, std_neg_dec)\n",
    "    # gaussian_pos_enc = dist.Normal(mean1_enc, std1_enc)\n",
    "    # gaussian_neg_enc = dist.Normal(mean2_enc, std2_enc)\n",
    "    # gaussian_pos_dec = dist.Normal(mean1_dec, std1_dec)\n",
    "    # gaussian_neg_dec = dist.Normal(mean2_dec, std2_dec)\n",
    "    \n",
    "    # 计算测试数据与正常样本的余弦相似度\n",
    "    test_enc = F.cosine_similarity(F.normalize(model(x_test)[0], p=2, dim=1), norm_enc.unsqueeze(0), dim=1)\n",
    "    test_dec = F.cosine_similarity(F.normalize(model(x_test)[1], p=2, dim=1), norm_dec.unsqueeze(0), dim=1)\n",
    "    # 使用解码器和编码器分别预测\n",
    "    y_pred_enc = torch.where(gaussian_pos_enc.log_prob(test_enc) > gaussian_neg_enc.log_prob(test_enc), 1, 0)\n",
    "    y_pred_dec = torch.where(gaussian_pos_dec.log_prob(test_dec) > gaussian_neg_dec.log_prob(test_dec), 1, 0)\n",
    "    # 投票预测\n",
    "    diff_enc = torch.abs(gaussian_pos_enc.log_prob(test_enc)-gaussian_neg_enc.log_prob(test_enc))\n",
    "    diff_dec = torch.abs(gaussian_pos_dec.log_prob(test_dec)-gaussian_neg_dec.log_prob(test_dec))\n",
    "    y_pred = torch.where(diff_enc > diff_dec, y_pred_enc, y_pred_dec)\n",
    "    return y_pred,y_pred_enc,y_pred_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始训练模型  \n",
    "用一小部分数据来训练模型  \n",
    "训练的数据来自于 online_x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=121, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): ReLU()\n",
       "    (1): Linear(in_features=32, out_features=64, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=64, out_features=121, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在线学习只使用20%的数据进行训练, 80%的数据用于在线学习和更新\n",
    "online_x_train, online_x_test, online_y_train, online_y_test = train_test_split(x_train, y_train, test_size=0.8, random_state=42)\n",
    "# 创建张量数据集\n",
    "train_dataset = TensorDataset(online_x_train, online_y_train)\n",
    "# 创建数据加载器\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "# 创建一个损失函数\n",
    "criterion = CRCLoss(device, tem)\n",
    "# 输入维度\n",
    "input_dim = x_train.shape[1]\n",
    "# 创建自编码器模型\n",
    "model = AE(input_dim).to(device)\n",
    "# 创建优化器\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "# 调整模型进入训练模式\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "809b52a31740410a8fea5b424c754c31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################## 初始化训练模型 ##########################\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    \n",
    "    #  j是批次索引，data是一个元组，包含输入和标签\n",
    "    for _ ,data in enumerate(train_loader,0):\n",
    "        # 获取输入和标签\n",
    "        inputs, labels = data # inputs.shape = (128, 206), labels.shape = (128,)\n",
    "        # 将标签移动到设备上\n",
    "        labels = labels.to(device)\n",
    "        # 优化器梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        # 前向传播\n",
    "        enc_features, dec_features = model(inputs.to(device))     \n",
    "        # 计算损失\n",
    "        loss=criterion(enc_features, labels)+criterion(dec_features, labels)\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        # 更新权重\n",
    "        optimizer.step()\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  将所有数据移动到设备上\n",
    "# x_train = x_train.to(device)\n",
    "# x_test = x_test.to(device)\n",
    "online_x_train, online_y_train  = online_x_train.to(device), online_y_train.to(device)\n",
    "# 克隆当前轮次的训练数据和测试数据\n",
    "x_train_this_epoch, x_test_left_epoch, y_train_this_epoch, y_test_left_epoch = online_x_train.clone(), online_x_test.clone().to(device), online_y_train.clone(), online_y_test.clone()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在线学习阶段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[1446   21]\n",
      " [   6 1311]]\n",
      "Accuracy  0.990301724137931\n",
      "Precision  0.9842342342342343\n",
      "Recall  0.9954441913439636\n",
      "F1 score  0.9898074745186863\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "227c717ff35c4a348983f35f5c5b3194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[1316  157]\n",
      " [   1 1310]]\n",
      "Accuracy  0.9432471264367817\n",
      "Precision  0.8929788684389911\n",
      "Recall  0.9992372234935164\n",
      "F1 score  0.9431245500359972\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c38ae10e7d7d47eab6eb565760f2aea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[1348  184]\n",
      " [   0 1252]]\n",
      "Accuracy  0.9339080459770115\n",
      "Precision  0.871866295264624\n",
      "Recall  1.0\n",
      "F1 score  0.9315476190476191\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a77c71cbd2f41c081e55a2c08398869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[1298  210]\n",
      " [   0 1276]]\n",
      "Accuracy  0.9245689655172413\n",
      "Precision  0.8586810228802153\n",
      "Recall  1.0\n",
      "F1 score  0.9239681390296887\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7399311d52247f9a435d028a62c642d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[1196  267]\n",
      " [   0 1321]]\n",
      "Accuracy  0.9040948275862069\n",
      "Precision  0.8318639798488665\n",
      "Recall  1.0\n",
      "F1 score  0.9082158817463045\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a3ab5217cab4ba6804ad97c16e9adf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[1189  273]\n",
      " [   0 1322]]\n",
      "Accuracy  0.9019396551724138\n",
      "Precision  0.8288401253918495\n",
      "Recall  1.0\n",
      "F1 score  0.9064106959204662\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "286a8c8799564c9cb27597ddd96fbfbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[1161  347]\n",
      " [   0 1276]]\n",
      "Accuracy  0.8753591954022989\n",
      "Precision  0.7861983980283426\n",
      "Recall  1.0\n",
      "F1 score  0.8803035529492929\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "854abea9359042bea58edb489f8d96fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[1163  303]\n",
      " [   0 1318]]\n",
      "Accuracy  0.8911637931034483\n",
      "Precision  0.8130783466995681\n",
      "Recall  1.0\n",
      "F1 score  0.8969037087444709\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da7205b5116d43448132cc02e5235913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[1085  381]\n",
      " [   0 1318]]\n",
      "Accuracy  0.8631465517241379\n",
      "Precision  0.7757504414361389\n",
      "Recall  1.0\n",
      "F1 score  0.8737156115346371\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be8494d2c0674e71a3e5f7dc03dea05b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[1052  423]\n",
      " [   0 1309]]\n",
      "Accuracy  0.8480603448275862\n",
      "Precision  0.7557736720554272\n",
      "Recall  1.0\n",
      "F1 score  0.8609010194015126\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7dc3dd4ef0d414f8db1152d8c8d6c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[1093  399]\n",
      " [   0 1292]]\n",
      "Accuracy  0.8566810344827587\n",
      "Precision  0.7640449438202247\n",
      "Recall  1.0\n",
      "F1 score  0.8662420382165605\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e7127db95a4ec9931adf85fa2592f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[ 998  471]\n",
      " [   0 1315]]\n",
      "Accuracy  0.8308189655172413\n",
      "Precision  0.7362821948488242\n",
      "Recall  1.0\n",
      "F1 score  0.8481135117703966\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e208bc2497574e0e8bdc25e8597d9b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[ 907  571]\n",
      " [   0 1306]]\n",
      "Accuracy  0.7948994252873564\n",
      "Precision  0.6957911561001598\n",
      "Recall  1.0\n",
      "F1 score  0.8206094879044926\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "465d2ce315ed46328bbbc57d0aee01c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[ 847  621]\n",
      " [   0 1316]]\n",
      "Accuracy  0.7769396551724138\n",
      "Precision  0.6794011357769747\n",
      "Recall  1.0\n",
      "F1 score  0.8090992929603443\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbcc16e3878b4c02b8ad979e4675f604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[ 926  648]\n",
      " [   0 1210]]\n",
      "Accuracy  0.7672413793103449\n",
      "Precision  0.6512378902045209\n",
      "Recall  1.0\n",
      "F1 score  0.788787483702738\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9f14beb4e064e62b1a428f063606984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[ 845  614]\n",
      " [   0 1325]]\n",
      "Accuracy  0.7794540229885057\n",
      "Precision  0.6833419288292935\n",
      "Recall  1.0\n",
      "F1 score  0.8118872549019608\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "043964deb1964ffeab131a942f0de051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[ 780  655]\n",
      " [   0 1349]]\n",
      "Accuracy  0.7647270114942529\n",
      "Precision  0.6731536926147704\n",
      "Recall  1.0\n",
      "F1 score  0.8046525499552639\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "358a0a65a4254a988a37734a0219875b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[ 791  675]\n",
      " [   0 1318]]\n",
      "Accuracy  0.7575431034482759\n",
      "Precision  0.6613146011038635\n",
      "Recall  1.0\n",
      "F1 score  0.7961340984596799\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc41f2439c31461d8a58ed83fda327b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[ 804  706]\n",
      " [   0 1274]]\n",
      "Accuracy  0.7464080459770115\n",
      "Precision  0.6434343434343435\n",
      "Recall  1.0\n",
      "F1 score  0.7830362630608482\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3cf776b2056492faa9348dc544d66ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[ 809  668]\n",
      " [   0 1307]]\n",
      "Accuracy  0.7600574712643678\n",
      "Precision  0.6617721518987342\n",
      "Recall  1.0\n",
      "F1 score  0.7964655697745278\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "990f988b82954db7981835dcbca4a875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[ 804  677]\n",
      " [   0 1303]]\n",
      "Accuracy  0.7568247126436781\n",
      "Precision  0.658080808080808\n",
      "Recall  1.0\n",
      "F1 score  0.7937861711848919\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95e11ba404a04df8b486bb2aa72b0756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[ 810  738]\n",
      " [   0 1236]]\n",
      "Accuracy  0.7349137931034483\n",
      "Precision  0.6261398176291794\n",
      "Recall  1.0\n",
      "F1 score  0.7700934579439253\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c9f15aaf38e4d2b87fbfcfa803c84bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[ 844  650]\n",
      " [   0 1290]]\n",
      "Accuracy  0.7665229885057471\n",
      "Precision  0.6649484536082474\n",
      "Recall  1.0\n",
      "F1 score  0.7987616099071208\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de0513b15094450b9ce85026ff4b8436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[ 849  621]\n",
      " [   0 1314]]\n",
      "Accuracy  0.7769396551724138\n",
      "Precision  0.6790697674418604\n",
      "Recall  1.0\n",
      "F1 score  0.8088642659279779\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f43acaf34544257a4eb0a7993570a53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[ 834  672]\n",
      " [   0 1278]]\n",
      "Accuracy  0.7586206896551724\n",
      "Precision  0.6553846153846153\n",
      "Recall  1.0\n",
      "F1 score  0.79182156133829\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "539d391e25fe40f8b2d28a2be439a470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[ 781  715]\n",
      " [   0 1288]]\n",
      "Accuracy  0.7431752873563219\n",
      "Precision  0.6430354468297553\n",
      "Recall  1.0\n",
      "F1 score  0.7827408082649651\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b668e11d36b142aba430320d574f2946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[ 828  717]\n",
      " [   0 1239]]\n",
      "Accuracy  0.7424568965517241\n",
      "Precision  0.6334355828220859\n",
      "Recall  1.0\n",
      "F1 score  0.7755868544600939\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c135670e7cf8489e89a40679672e5d98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[ 772  700]\n",
      " [   0 1312]]\n",
      "Accuracy  0.7485632183908046\n",
      "Precision  0.6520874751491054\n",
      "Recall  1.0\n",
      "F1 score  0.789410348977136\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ece47ea230bf4d4c89b2fa2e9ff74fba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[ 752  720]\n",
      " [   0 1312]]\n",
      "Accuracy  0.7413793103448276\n",
      "Precision  0.6456692913385826\n",
      "Recall  1.0\n",
      "F1 score  0.784688995215311\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a17ff2849e344ac58c31178f6ea135d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[ 768  753]\n",
      " [   0 1263]]\n",
      "Accuracy  0.7295258620689655\n",
      "Precision  0.6264880952380952\n",
      "Recall  1.0\n",
      "F1 score  0.7703568161024703\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "771e940d1b0d44b5b019df3fd2294a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[ 781  728]\n",
      " [   0 1275]]\n",
      "Accuracy  0.7385057471264368\n",
      "Precision  0.63654518222666\n",
      "Recall  1.0\n",
      "F1 score  0.7779133618059793\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f067a7a94cf54c8d9d11272cec01caec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[ 756  714]\n",
      " [   0 1314]]\n",
      "Accuracy  0.7435344827586207\n",
      "Precision  0.6479289940828402\n",
      "Recall  1.0\n",
      "F1 score  0.7863554757630161\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "928bcfaa5003486395e0b9f5fc20ca90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[ 770  690]\n",
      " [   0 1324]]\n",
      "Accuracy  0.7521551724137931\n",
      "Precision  0.6573982125124131\n",
      "Recall  1.0\n",
      "F1 score  0.793289394847214\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03066b3bde5a4ae8b277dda98522d5ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[ 749  746]\n",
      " [   0 1289]]\n",
      "Accuracy  0.7320402298850575\n",
      "Precision  0.6334152334152334\n",
      "Recall  1.0\n",
      "F1 score  0.7755716004813478\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ffd8dadaab4aaabfb9fe03b5445db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[ 752  737]\n",
      " [   0 1295]]\n",
      "Accuracy  0.7352729885057471\n",
      "Precision  0.6373031496062992\n",
      "Recall  1.0\n",
      "F1 score  0.7784791103095882\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99072faed04f4f8d9fd0384d9d064d6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[ 798  682]\n",
      " [   0 1304]]\n",
      "Accuracy  0.7550287356321839\n",
      "Precision  0.6565961732124874\n",
      "Recall  1.0\n",
      "F1 score  0.7927051671732522\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb2a7eb0814346ed82aaeb25ca90c16a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[145 158]\n",
      " [  0 252]]\n",
      "Accuracy  0.7153153153153153\n",
      "Precision  0.6146341463414634\n",
      "Recall  1.0\n",
      "F1 score  0.7613293051359517\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60607e8c851e41618ee3835caeafbe6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 在线训练的过程，需要不断的测试新输入的数据，每次来一个新的数据，就需要测试一次，预测出标签，同时做一个累积，每当累积到一定数量的数据时，就需要微调模型\n",
    "while len(x_test_left_epoch) > 0:\n",
    "    # 如果剩余的数据少于采样间隔，则将所有剩余的数据作为这一轮的测试数据\n",
    "    if len(x_test_left_epoch) < sample_interval:\n",
    "        x_test_this_epoch = x_test_left_epoch.clone()\n",
    "        x_test_left_epoch.resize_(0)\n",
    "        y_test_this_epoch = y_test_left_epoch.clone()\n",
    "        y_test_left_epoch.resize_(0)\n",
    "        \n",
    "    # 否则，从剩余的数据中采样一部分作为这一轮的测试数据\n",
    "    else:\n",
    "        x_test_this_epoch = x_test_left_epoch[:sample_interval].clone()\n",
    "        x_test_left_epoch = x_test_left_epoch[sample_interval:]\n",
    "        y_test_this_epoch = y_test_left_epoch[:sample_interval].clone()\n",
    "        y_test_left_epoch = y_test_left_epoch[sample_interval:]\n",
    "    \n",
    "    # 据训练集里的正常样本的特征得出一个平均正常特征\n",
    "    ## online_y_train == 0是正常样本的标签\n",
    "    ## online_x_train[(online_y_train == 0)].shape   torch.Size([11190, 206]) 之所以要加squeeze()，是为了防止避免下面的情况[1,11190,206]\n",
    "    ## 论文里有提到，即采用编码器的输出，也采用解码器的输出\n",
    "    normal_enc = torch.mean(F.normalize(model(online_x_train[(online_y_train == 0).squeeze()])[0], p=2, dim=1), dim=0)\n",
    "    normal_dec = torch.mean(F.normalize(model(online_x_train[(online_y_train == 0).squeeze()])[1], p=2, dim=1), dim=0)\n",
    "    # 预测标签\n",
    "    predict_label,_,_ = predict(normal_enc, normal_dec, x_train_this_epoch, y_train_this_epoch, x_test_this_epoch,model)\n",
    "    # 评估准确性\n",
    "    evaluate(y_test_this_epoch, predict_label)\n",
    "    \n",
    "    # 随机翻转\n",
    "    num_flips = int(flip_percent * len(predict_label))\n",
    "    shuffle_index = np.random.choice(len(predict_label), num_flips, replace=False)\n",
    "    # 翻转标签\n",
    "    flip_label = predict_label.clone()\n",
    "    flip_label[shuffle_index] = 1 - flip_label[shuffle_index]\n",
    "    flip_label = flip_label.to(device)\n",
    "    # 更新数据集\n",
    "    x_train_this_epoch = torch.cat((x_train_this_epoch, x_test_this_epoch), 0)\n",
    "    y_train_this_epoch = torch.cat((y_train_this_epoch, flip_label), 0)\n",
    "    \n",
    "    train_ds=TensorDataset(x_train_this_epoch, y_train_this_epoch)\n",
    "    train_dl=DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "    \n",
    "    for epoch in tqdm(range(epoch_online)):\n",
    "        for _ ,data in enumerate(train_dl,0):\n",
    "            inputs, labels = data\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            enc_features, dec_features = model(inputs.to(device))\n",
    "            loss = criterion(enc_features, labels) + criterion(dec_features, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评估模型（在测试集上训练）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[ 6116  3595]\n",
      " [    1 12832]]\n",
      "Accuracy  0.8404897090134847\n",
      "Precision  0.7811529798502466\n",
      "Recall  0.9999220758980752\n",
      "F1 score  0.8771018455228982\n"
     ]
    }
   ],
   "source": [
    "normal_enc = torch.mean(F.normalize(model(online_x_train[(online_y_train == 0).squeeze()])[0], p=2, dim=1), dim=0)\n",
    "normal_dec = torch.mean(F.normalize(model(online_x_train[(online_y_train == 0).squeeze()])[1], p=2, dim=1), dim=0)\n",
    "x_test = x_test.to(device)\n",
    "y_pred,_,_ = predict(normal_enc, normal_dec, x_train_this_epoch,y_train_this_epoch, x_test, model)\n",
    "evaluate(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 零日攻击检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('en_KDDTrain+.csv')\n",
    "df_test = pd.read_csv('en_KDDTest+.csv')\n",
    "# 归一化\n",
    "exclude_columns=['label','label2','class']\n",
    "normalize_columns = [c for c in df_train.columns if c not in exclude_columns]\n",
    "\n",
    "normalize = MinMaxScaler().fit(df_test[normalize_columns])\n",
    "df_test[normalize_columns] = normalize.transform(df_test[normalize_columns])\n",
    "\n",
    "\n",
    "\n",
    "attacks_train = set(df_train['label'].unique())\n",
    "attacks_test = set(df_test['label'].unique())\n",
    "# 仅仅在测试集中出现的攻击类别\n",
    "attacks_only_test = attacks_test - attacks_train\n",
    "indices_unseen = df_test[df_test['label'].isin(attacks_only_test)].index\n",
    "# 在训练集和测试集中都出现的攻击类别\n",
    "indices_seen = df_test[df_test['label'].isin(attacks_train&attacks_test)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seen = df_test.loc[indices_seen]\n",
    "df_unseen = df_test.loc[indices_unseen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_dos = df_seen[df_seen['class']=='DoS']\n",
    "seen_probe = df_seen[df_seen['class']=='Probe']\n",
    "seen_r2l = df_seen[df_seen['class']=='R2L']\n",
    "seen_u2r = df_seen[df_seen['class']=='U2R']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_dos = df_unseen[df_unseen['class']=='DoS']\n",
    "unseen_probe = df_unseen[df_unseen['class']=='Probe']\n",
    "unseen_r2l = df_unseen[df_unseen['class']=='R2L']\n",
    "unseen_u2r = df_unseen[df_unseen['class']=='U2R']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_df(df):\n",
    "    x=torch.FloatTensor(df.drop(columns=['label','label2','class']).to_numpy()).to(device)\n",
    "    y=torch.LongTensor(df['label2'].to_numpy()).to(device)\n",
    "    y_pred,_,_ = predict(normal_enc, normal_dec, x_train_this_epoch, y_train_this_epoch, x, model)\n",
    "    # print(y_pred)\n",
    "    evaluate(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DoS', 'normal', 'Probe', 'R2L', 'U2R'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[5741]]\n",
      "Accuracy  1.0\n",
      "Precision  1.0\n",
      "Recall  1.0\n",
      "F1 score  1.0\n",
      "Confusion matrix\n",
      "[[1106]]\n",
      "Accuracy  1.0\n",
      "Precision  1.0\n",
      "Recall  1.0\n",
      "F1 score  1.0\n",
      "Confusion matrix\n",
      "[[2199]]\n",
      "Accuracy  1.0\n",
      "Precision  1.0\n",
      "Recall  1.0\n",
      "F1 score  1.0\n",
      "Confusion matrix\n",
      "[[37]]\n",
      "Accuracy  1.0\n",
      "Precision  1.0\n",
      "Recall  1.0\n",
      "F1 score  1.0\n"
     ]
    }
   ],
   "source": [
    "predict_df(seen_dos)\n",
    "predict_df(seen_probe)\n",
    "predict_df(seen_r2l)\n",
    "predict_df(seen_u2r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[   0    0]\n",
      " [   1 1716]]\n",
      "Accuracy  0.9994175888177053\n",
      "Precision  1.0\n",
      "Recall  0.9994175888177053\n",
      "F1 score  0.9997087095834547\n",
      "Confusion matrix\n",
      "[[1315]]\n",
      "Accuracy  1.0\n",
      "Precision  1.0\n",
      "Recall  1.0\n",
      "F1 score  1.0\n",
      "Confusion matrix\n",
      "[[555]]\n",
      "Accuracy  1.0\n",
      "Precision  1.0\n",
      "Recall  1.0\n",
      "F1 score  1.0\n",
      "Confusion matrix\n",
      "[[163]]\n",
      "Accuracy  1.0\n",
      "Precision  1.0\n",
      "Recall  1.0\n",
      "F1 score  1.0\n"
     ]
    }
   ],
   "source": [
    "predict_df(unseen_dos)\n",
    "predict_df(unseen_probe)\n",
    "predict_df(unseen_r2l)\n",
    "predict_df(unseen_u2r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_xjy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
